{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Naive Bayes**"
      ],
      "metadata": {
        "id": "z0Jdu_NGlZzA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcKT1TJVc2Nn",
        "outputId": "f2ca8198-4cf7-4a85-cdd2-1b9668a2bef6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#train\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn import preprocessing\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load the training data\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Machine Project 2023/heart_disease_health_indicators_BRFSS2015.csv')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "X1 = df.drop(['HeartDiseaseorAttack'] , axis = 1)\n",
        "\n",
        "y1 = np.array(df['HeartDiseaseorAttack'])\n",
        "\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split (X1, y1, test_size=0.25)\n",
        "\n",
        "\n",
        "\n",
        "clf = GaussianNB()\n",
        "clf.fit(X_train1, y_train1)\n",
        "\n",
        "\n",
        "y_pred1 = clf.predict(X_test1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM"
      ],
      "metadata": {
        "id": "GY5ooX1pmRVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn import preprocessing\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Machine Project 2023/heart_disease_health_indicators_BRFSS2015.csv')\n",
        "\n",
        "X2 = df.drop(['HeartDiseaseorAttack'] , axis = 1)\n",
        "y2 = np.array(df['HeartDiseaseorAttack'])\n",
        "\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.25, random_state=42)\n",
        "\n",
        "print(len(X_train2))\n",
        "print(len(X_test2))\n",
        "\n",
        "\n",
        "svm = SVC(C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)\n",
        "\n",
        "\n",
        "svm.fit(X_train2, y_train2)\n",
        "\n",
        "y_pred2 = svm.predict(X_test2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvMbd7ybe1q6",
        "outputId": "40c05b8e-c50c-4d50-e278-2ed2cdfc74bb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "190260\n",
            "63420\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "z7TJBmll7qQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **GradientBoostingClassifier**"
      ],
      "metadata": {
        "id": "DvenkGFdm0Lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Train\n",
        "\n",
        "# Import models and utility functions\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Setting SEED for reproducibility\n",
        "SEED = 23\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Machine Project 2023/heart_disease_health_indicators_BRFSS2015.csv')\n",
        "df\n",
        "x3=df.drop(['HeartDiseaseorAttack'],axis=1)\n",
        "x3\n",
        "y3=np.array(df['HeartDiseaseorAttack'])\n",
        "# Importing the dataset\n",
        "\n",
        " \n",
        "# Splitting dataset\n",
        "train_x3, test_x3, train_y3, test_y3 = train_test_split(x3, y3,\n",
        "                                                    test_size = 0.25,\n",
        "                                                    random_state = SEED)\n",
        " \n",
        "# Instantiate Gradient Boosting Regressor\n",
        "gbc = GradientBoostingClassifier(n_estimators=300,\n",
        "                                 learning_rate=0.05,\n",
        "                                 random_state=100,\n",
        "                                 max_features=5 )\n",
        "# Fit to training set\n",
        "gbc.fit(train_x3, train_y3)\n",
        " \n",
        "# Predict on test set\n",
        "pred_y3= gbc.predict(test_x3)\n",
        " \n",
        "from sklearn import preprocessing\n",
        "  \n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "y3 = label_encoder.fit_transform(y3)\n",
        "test_y3 = label_encoder.fit_transform(test_y3)\n",
        "pred_y3 = label_encoder.fit_transform(pred_y3)"
      ],
      "metadata": {
        "id": "RRypkRPshxt4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **MLP**"
      ],
      "metadata": {
        "id": "VhB69rBbnPO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load sample data\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Machine Project 2023/heart_disease_health_indicators_BRFSS2015.csv')\n",
        "df\n",
        "x4=df.drop(['HeartDiseaseorAttack'],axis=1)\n",
        "x4\n",
        "y4=np.array(df['HeartDiseaseorAttack'])\n",
        "x_train4,x_test4,y_train4,y_test4=train_test_split(x4,y4,test_size=0.25)\n",
        "# Create an MLP classifier\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100, ),        # Number of hidden layers and units per layer\n",
        "                    activation='relu',                 # Activation function ('identity', 'logistic', 'tanh', 'relu')\n",
        "                    solver='adam',                     # Solver for weight optimization ('lbfgs', 'sgd', 'adam')\n",
        "                    alpha=0.0001,                      # L2 penalty (regularization term) parameter\n",
        "                    batch_size='auto',                 # Size of minibatches for stochastic optimizers\n",
        "                    learning_rate='adaptive',          # Learning rate schedule ('constant', 'invscaling', 'adaptive')\n",
        "                    learning_rate_init=0.005,          # The initial learning rate\n",
        "                    power_t=0.5,                       # The exponent for inverse scaling learning rate\n",
        "                    max_iter=200,                      # Maximum number of iterations\n",
        "                    shuffle=True,                      # Whether to shuffle samples in each iteration\n",
        "                    random_state=None,                 # Seed for the random number generator\n",
        "                    tol=0.0001,                        # Tolerance for the optimization\n",
        "                    verbose=False,                     # Whether to print progress messages\n",
        "                    warm_start=False,                  # Reuse the previous solution\n",
        "                    momentum=0.9,                      # Momentum for gradient descent update\n",
        "                    nesterovs_momentum=True,           # Whether to use Nesterov's momentum\n",
        "                    early_stopping=False,              # Terminate training when validation score is not improving\n",
        "                    validation_fraction=0.1,           # Proportion of training data to set aside as validation set\n",
        "                    beta_1=0.9,                        # Exponential decay rate for estimates of first moment vector in adam\n",
        "                    beta_2=0.999,                      # Exponential decay rate for estimates of second moment vector in adam\n",
        "                    epsilon=1e-8,                      # Value for numerical stability in adam\n",
        "                    n_iter_no_change=10,               # Maximum number of epochs without any improvement in the loss\n",
        "                    max_fun=15000)                     # Maximum number of function calls for the solver\n",
        "\n",
        "# Train the MLP classifier\n",
        "mlp.fit(x_train4, y_train4)\n",
        "\n",
        "# Make predictions\n",
        "y_pred4 = mlp.predict(x_test4)\n",
        "\n",
        "\n",
        "from sklearn import preprocessing\n",
        "  \n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "y4 = label_encoder.fit_transform(y4)\n",
        "test_y4 = label_encoder.fit_transform(y_test4)\n",
        "pred_y4 = label_encoder.fit_transform(y_pred4)"
      ],
      "metadata": {
        "id": "iZ1EoWnxiwmY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **logistic regression**"
      ],
      "metadata": {
        "id": "mIPn_zOBnZwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Machine Project 2023/heart_disease_health_indicators_BRFSS2015.csv')\n",
        "\n",
        "X5 = data.drop(['HeartDiseaseorAttack'] , axis = 1)\n",
        "y5 = np.array(data['HeartDiseaseorAttack'])\n",
        "\n",
        "\n",
        "X_train5,X_test5,y_train5,y_test5 = train_test_split(X5,y5,test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "classifier = LogisticRegression(solver='lbfgs',random_state=0)\n",
        "classifier.fit(X_train5, y_train5)\n",
        "\n",
        "\n",
        "\n",
        "predicted_y5 = classifier.predict(X_test5)\n",
        "from sklearn import preprocessing\n",
        "  \n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "y5 = label_encoder.fit_transform(y5)\n",
        "test_y5 = label_encoder.fit_transform(y_test5)\n",
        "pred_y5 = label_encoder.fit_transform(predicted_y5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lb0RWO2YlWq4",
        "outputId": "250f24d0-087f-44e6-e6b0-638dfb4c8ffa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **KNN**"
      ],
      "metadata": {
        "id": "lNCcQRP1nwKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn import preprocessing\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "# Load the training data\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Machine Project 2023/heart_disease_health_indicators_BRFSS2015.csv')\n",
        "\n",
        "# Create feature and target arrays\n",
        "X6 = X = df.drop(['HeartDiseaseorAttack'] , axis = 1)\n",
        "\n",
        "y6 = np.array(df['HeartDiseaseorAttack'])\n",
        "\n",
        "\n",
        "\n",
        "# Split into training and test set\n",
        "X_train6, X_test6, y_train6, y_test6 = train_test_split(\n",
        "             X6, y6, test_size = 0.25, random_state=42)\n",
        "  \n",
        "neighbors = np.arange(1, 9)\n",
        "train_accuracy = np.empty(len(neighbors))\n",
        "test_accuracy = np.empty(len(neighbors))\n",
        "  \n",
        "\n",
        "\n",
        "# Loop over K values\n",
        "\n",
        "#for i, k in enumerate(neighbors):\n",
        "#    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "#    knn.fit(X_train, y_train)\n",
        "      \n",
        "#    # Compute training and test data accuracy\n",
        " #   train_accuracy[i] = knn.score(X_train, y_train)\n",
        "  #  test_accuracy[i] = knn.score(X_test, y_test)\n",
        "\n",
        "  \n",
        "\n",
        "knn = KNeighborsClassifier(21)\n",
        "knn.fit(X_train6, y_train6)\n",
        "# Predict on test set\n",
        "pred_y6 = knn.predict(X_test6)\n",
        " \n",
        "from sklearn import preprocessing\n",
        "  \n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "y6 = label_encoder.fit_transform(y6)\n",
        "test_y6 = label_encoder.fit_transform(y_test6)\n",
        "pred_y6 = label_encoder.fit_transform(pred_y6)"
      ],
      "metadata": {
        "id": "TKb9rT6xoGr7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes TEST\n"
      ],
      "metadata": {
        "id": "JBtPPmR9lw8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "  \n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "y1 = label_encoder.fit_transform(y1)\n",
        "y_test1 = label_encoder.fit_transform(y_test1)\n",
        "\n",
        "y_pred1 = label_encoder.fit_transform(y_pred1)\n",
        "\n",
        "accuracy1 = accuracy_score(y_test1, y_pred1)\n",
        "print('Accuracy:', accuracy1)\n",
        "recall1 = recall_score(y_test1, y_pred1)\n",
        "print('Recall:', recall1)\n",
        "precision1 = precision_score(y_test1, y_pred1)\n",
        "print('Precision:', precision1)\n",
        "f11 = f1_score(y_test1,y_pred1)\n",
        "print('F1_score:', f11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itLXgdkSlwPL",
        "outputId": "4321b9a0-908d-4fbb-98e5-6b47072cf6e8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8183538315988647\n",
            "Recall: 0.5501509560550151\n",
            "Precision: 0.2706717280079221\n",
            "F1_score: 0.36283185840707965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***SVM TEST***"
      ],
      "metadata": {
        "id": "2Nq5e7V0mcp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy2 = accuracy_score(y_test2, y_pred2)\n",
        "print('Accuracy:', accuracy2)\n",
        "recall2 = recall_score(y_test2, y_pred2,pos_label=1)\n",
        "print('Recall:', recall2)\n",
        "precision2 = precision_score(y_test2, y_pred2,pos_label=1)\n",
        "print('Precision:', precision2)\n",
        "f12 = f1_score(y_test2,y_pred2,pos_label=1)\n",
        "print('F1_score:', f12)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqNElGCTmdPI",
        "outputId": "804e21ce-1773-46d6-dcd4-ad1c89a21f1f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.906401766004415\n",
            "Recall: 0.0\n",
            "Precision: 0.0\n",
            "F1_score: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***GradientBoostingClassifier TEST***\n"
      ],
      "metadata": {
        "id": "Bht3TCpxm-Zr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy3 = accuracy_score(test_y3, pred_y3)\n",
        "print('Accuracy:', accuracy3)\n",
        "recall3 = recall_score(test_y3, pred_y3)\n",
        "print('Recall:', recall3)\n",
        "precision3 = precision_score(test_y3, pred_y3)\n",
        "print('Precision:', precision3)\n",
        "f13 = f1_score(test_y3,pred_y3)\n",
        "print('F1_score:', f13)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVBTTYN6m-34",
        "outputId": "c3c2f1a5-4376-4661-bbf8-0bb157b1a855"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9077262693156732\n",
            "Recall: 0.11639591358231452\n",
            "Precision: 0.5468135326514555\n",
            "F1_score: 0.191935929301298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***MLP TEST***"
      ],
      "metadata": {
        "id": "jNsTXM53nKZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy4 = accuracy_score(test_y4, pred_y4)\n",
        "print('Accuracy:', accuracy4)\n",
        "recall4 = recall_score(test_y4, pred_y4)\n",
        "print('Recall:', recall4)\n",
        "precision4= precision_score(test_y4, pred_y4)\n",
        "print('Precision:', precision4)\n",
        "f14 = f1_score(test_y4,pred_y4)\n",
        "print('F1_score:', f14)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1oX2njpnK14",
        "outputId": "70ce7c8c-2c9d-4754-c43c-bca167180e86"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9070167139703563\n",
            "Recall: 0.09656545545047288\n",
            "Precision: 0.562862669245648\n",
            "F1_score: 0.1648491715054525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Logistic Regression TEST***"
      ],
      "metadata": {
        "id": "vIxwfH4enf6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy5 = accuracy_score(test_y5, pred_y5)\n",
        "print('Accuracy:', accuracy5)\n",
        "recall5 = recall_score(test_y5, pred_y5)\n",
        "print('Recall:', recall5)\n",
        "precision5= precision_score(test_y5, pred_y5)\n",
        "print('Precision:', precision5)\n",
        "f15 = f1_score(test_y5,pred_y5)\n",
        "print('F1_score:', f15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQ2NvbM0ngPH",
        "outputId": "257acfee-7ed1-4c0c-b294-c269f973bcea"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9072216966256701\n",
            "Recall: 0.11994609164420485\n",
            "Precision: 0.5189504373177842\n",
            "F1_score: 0.1948549534756431\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***KNN TEST***"
      ],
      "metadata": {
        "id": "CBAfVNmYnpOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy6 = accuracy_score(test_y6, pred_y6)\n",
        "print('Accuracy:', accuracy6)\n",
        "recall6 = recall_score(test_y6, pred_y6)\n",
        "print('Recall:', recall6)\n",
        "precision6 = precision_score(test_y6, pred_y6)\n",
        "print('Precision:', precision6)\n",
        "f16 = f1_score(test_y6,pred_y6)\n",
        "print('F1_score:', f16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BctPFQfno8j",
        "outputId": "00222675-e8bb-4ec2-9b1b-1c355c3f1835"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9062756228319142\n",
            "Recall: 0.026785714285714284\n",
            "Precision: 0.48773006134969327\n",
            "F1_score: 0.050782497604599165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***All Results***"
      ],
      "metadata": {
        "id": "tu98adWtmC6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "table = [[\"Classsifier name\", \"Accuracy\", \"Recall\", \"Precision\",\"f1-score\"], \n",
        "         [\"Naive bayes\", accuracy1, recall1, precision1,f11], \n",
        "         [\"SVM\", accuracy2, recall2, precision2,f12], \n",
        "         [\"Gradient Boosting\", accuracy3, recall3, precision3,f13],\n",
        "         [\"MLP\", accuracy4, recall4, precision4,f14],\n",
        "         [\"Logistic regression\", accuracy5, recall5, precision5,f15],\n",
        "         [\"KNN\", accuracy6, recall6, precision6,f16]\n",
        "         ]\n",
        "\n",
        "\n",
        "\n",
        "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTKiKve_a-h4",
        "outputId": "cdab331a-e317-4871-f94c-14fcfc13bc9c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "╒═════════════════════╤════════════╤═══════════╤═════════════╤════════════╕\n",
            "│ Classsifier name    │   Accuracy │    Recall │   Precision │   f1-score │\n",
            "╞═════════════════════╪════════════╪═══════════╪═════════════╪════════════╡\n",
            "│ Naive bayes         │   0.818354 │ 0.550151  │    0.270672 │  0.362832  │\n",
            "├─────────────────────┼────────────┼───────────┼─────────────┼────────────┤\n",
            "│ SVM                 │   0.906402 │ 0         │    0        │  0         │\n",
            "├─────────────────────┼────────────┼───────────┼─────────────┼────────────┤\n",
            "│ Gradient Boosting   │   0.907726 │ 0.116396  │    0.546814 │  0.191936  │\n",
            "├─────────────────────┼────────────┼───────────┼─────────────┼────────────┤\n",
            "│ MLP                 │   0.907017 │ 0.0965655 │    0.562863 │  0.164849  │\n",
            "├─────────────────────┼────────────┼───────────┼─────────────┼────────────┤\n",
            "│ Logistic regression │   0.907222 │ 0.119946  │    0.51895  │  0.194855  │\n",
            "├─────────────────────┼────────────┼───────────┼─────────────┼────────────┤\n",
            "│ KNN                 │   0.906276 │ 0.0267857 │    0.48773  │  0.0507825 │\n",
            "╘═════════════════════╧════════════╧═══════════╧═════════════╧════════════╛\n"
          ]
        }
      ]
    }
  ]
}